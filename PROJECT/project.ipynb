{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed004b6a",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "# Your task is to implement a forecasting system as Python script (i.e. a .py file). It should be able to forecast up to 5 time series and dynamically select suitable models for each time series according to some logic. You as expert team are tasked with devising a simple logic.\n",
    "# At the top of your script, you should allow your users to set some variables as input:\n",
    "# • file_name: str, file name of data to be read in.\n",
    "# • ids_to_forecast: list[str], time series names of time series selected to be forecasted.\n",
    "# • metric: str, either 'mape' or 'mse'.\n",
    "# If the input for metric is not correct, throw an error and stop the execution.\n",
    "# Your forecasting system then should do the following:\n",
    "# a) Read in the data as given in proj1_exampleinput.csv.\n",
    "# b) Perform any necessary data preparation and filter the data such that you obtain the specified time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "128e083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import sys  # Needed to exit the script in case of error\n",
    "\n",
    "\n",
    "# --------------------- USER INPUT ---------------------\n",
    "file_name = 'proj1_exampleinput.csv'\n",
    "ids_to_forecast = ['C1002', 'C1050', 'C1080', 'C1016', 'C1126']\n",
    "metric = 'mape'  # 'mape' or 'mse'\n",
    "# -----------------------------------------------------\n",
    "\n",
    "# Validate the metric input\n",
    "valid_metrics = ['mape', 'mse']\n",
    "if metric not in valid_metrics:\n",
    "    raise ValueError(f\"❌ Invalid metric '{metric}'. Please choose either 'mape' or 'mse'.\")\n",
    "\n",
    "# Read and process data into Nixtla format\n",
    "data = pd.read_csv(file_name, parse_dates=['Month'])\n",
    "\n",
    "# Filter to include only the selected time series\n",
    "data = data[data['product_class'].isin(ids_to_forecast)]\n",
    "\n",
    "# Sort by ID and timestamp\n",
    "data = data.sort_values(by=['product_class', 'Month'])\n",
    "\n",
    "# Rename columns to match Nixtla format\n",
    "data = data.rename(columns={'product_class': 'unique_id', 'Month': 'ds', 'sales_volume': 'y'})\n",
    "\n",
    "# Reset index for clean structure\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fe245a",
   "metadata": {},
   "source": [
    "# c) Select the models for forecasting using the following logic. For each category - if not explicitly specified differently - use 2-4 suitable models from the lecture:\n",
    "# • Category 1: Time series is shorter than two seasonality periods (pick only one model).\n",
    "# • Category 2: Else, time series contains zeroes and has high seasonality.\n",
    "# • Category 3: Else, time series has high seasonality and only positive values.\n",
    "# • Category 4: Times series has low or no seasonality and zeroes.\n",
    "# • Category 5: All remaining.\n",
    "# • Additionally, in all cases always run the Naive forecast as benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a43f2bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series C1002 assigned to Category 5 with models: ['Naive', 'AutoARIMA', 'AutoETS', 'HistoricAverage', 'RandomWalkWithDrift']\n",
      "Series C1016 assigned to Category 5 with models: ['Naive', 'AutoARIMA', 'AutoETS', 'HistoricAverage', 'RandomWalkWithDrift']\n",
      "Series C1050 assigned to Category 1 with models: ['Naive', 'HistoricAverage']\n",
      "Series C1080 assigned to Category 3 with models: ['Naive', 'SeasonalNaive', 'SeasonalWindowAverage', 'AutoARIMA', 'AutoETS']\n",
      "Series C1126 assigned to Category 5 with models: ['Naive', 'AutoARIMA', 'AutoETS', 'HistoricAverage', 'RandomWalkWithDrift']\n"
     ]
    }
   ],
   "source": [
    "from statsforecast.models import (\n",
    "    Naive,\n",
    "    HistoricAverage,\n",
    "    RandomWalkWithDrift,\n",
    "    SeasonalNaive,\n",
    "    SeasonalWindowAverage,\n",
    "    AutoETS,\n",
    "    HoltWinters,\n",
    "    AutoARIMA\n",
    ")\n",
    "\n",
    "\n",
    "# --------------------- Model selection by category ---------------------\n",
    "def has_high_seasonality(series):\n",
    "    result = seasonal_decompose(series, model='additive', period=12, extrapolate_trend='freq')\n",
    "    seasonal_var = np.var(result.seasonal.dropna())\n",
    "    resid_var = np.var(result.resid.dropna())\n",
    "    return seasonal_var > resid_var\n",
    "\n",
    "\n",
    "def is_positive(series):\n",
    "    return series.min() > 0\n",
    "\n",
    "def has_zeroes(series):\n",
    "    return (series == 0).any()\n",
    "\n",
    "def get_category(ts):\n",
    "    if len(ts) < 24:\n",
    "        return 1\n",
    "    elif has_zeroes(ts) and has_high_seasonality(ts):\n",
    "        return 2\n",
    "    elif has_high_seasonality(ts) and is_positive(ts):\n",
    "        return 3\n",
    "    elif not has_high_seasonality(ts) and has_zeroes(ts):\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "\n",
    "# Selected models by category with justification:\n",
    "category_models = {\n",
    "    1: [\n",
    "        # Category 1: Shorter than two seasonality periods\n",
    "        # Models:\n",
    "        # - Naive: Serves as a baseline for all forecasting tasks. It assumes the next value is the last observed one. Particularly useful when data is limited.\n",
    "        # - HistoricAverage: Calculates the mean of historical values. Ideal when no seasonal or trend patterns are observable due to short history.\n",
    "        # - Reasoning: Since there is not enough data, complex models may not be able to capture seasonality or trends optimally. Simple models can work with short ranges.\n",
    "        \n",
    "        Naive(),\n",
    "        HistoricAverage()\n",
    "    ],\n",
    "    2: [\n",
    "        # Category 2: High seasonality + zero values\n",
    "        # Models:\n",
    "        # - Naive: Always included as a Benchmark.\n",
    "        # - SeasonalNaive: Projects the value from the same point in the previous season. Strong benchmark when seasonality dominates.\n",
    "        # - SeasonalWindowAverage: Smooths seasonal patterns by averaging same-period values from previous seasons, reducing variance.\n",
    "        # - SARIMA: Handles both seasonality and trend components. Can model autoregression, moving averages, and seasonal differencing.\n",
    "        # - Holt-Winters (Additive): Suitable for capturing trend and seasonality when values can reach zero — additive form avoids issues that multiplicative models would face with zeroes.\n",
    "        # - Reasoning: When seasonality is present and the data includes zero values (e.g., sales data with no sales in some periods), additive models are safer than multiplicative ones, which assume strictly positive values.\n",
    "        # The SeasonalNaive and SeasonalWindowAverage models exploit the strong seasonal signals, while SARIMA and Additive Holt-Winters are more robust and flexible for detecting seasonal patterns and underlying trends.\n",
    "\n",
    "        Naive(),\n",
    "        SeasonalNaive(season_length=12),\n",
    "        SeasonalWindowAverage(window_size=3, season_length=12),\n",
    "        AutoARIMA(season_length=12, alias='SARIMA'),\n",
    "        HoltWinters(season_length=12, error_type='A', alias='HW_Add')  # Additive\n",
    "    ],\n",
    "    3: [\n",
    "        # Category 3: High seasonality + positive values\n",
    "        # Models:\n",
    "        # - Naive: Always included as a Benchmark.\n",
    "        # - SeasonalNaive: A reliable baseline for data with regular seasonality.\n",
    "        # - SeasonalWindowAverage: Averages values from the same season across previous cycles to smooth irregularities.\n",
    "        # - SARIMA: Capable of capturing both trend and seasonality via seasonal ARIMA formulation.\n",
    "        # - AutoETS: Automatically selects between additive and multiplicative exponential smoothing. Since values are strictly positive, multiplicative Holt-Winters can be considered, which better captures proportional seasonal effects.\n",
    "        # - Reasoning: With strong seasonality and no zero values, multiplicative models become viable. They model seasonal effects as a proportion of the level (e.g., summer sales being 1.5× baseline).\n",
    "        # AutoETS explores this automatically. SARIMA is useful when autocorrelation structures are complex. Combined, these models cover a spectrum from simplicity (Naive) to flexibility (SARIMA/AutoETS).\n",
    "\n",
    "        Naive(),\n",
    "        SeasonalNaive(season_length=12),\n",
    "        SeasonalWindowAverage(window_size=3, season_length=12),\n",
    "        AutoARIMA(season_length=12, alias='SARIMA'),\n",
    "        AutoETS(model='ZZZ', alias='AutoETS')  # Includes additive and multiplicative HW\n",
    "    ],\n",
    "    4: [\n",
    "        # Category 4: Low seasonality + zero values\n",
    "        # Models:\n",
    "        # - Naive: Always included as a Benchmark.\n",
    "        # - HistoricAverage: Effective when values fluctuate around a stable mean.\n",
    "        # - RandomWalkWithDrift: Assumes the series follows a trend with random variation. Useful when there's no seasonality but a clear directional movement.\n",
    "        # - AutoETS (Simple/Double Exponential Smoothing): Simple ES assumes no trend/seasonality; double ES introduces trend while maintaining simplicity.\n",
    "        # - ARIMA: General-purpose model that can capture linear trends and autocorrelation without requiring seasonality.\n",
    "        # - Reasoning: In the absence of strong seasonality, models that focus on level and trend (rather than seasonal cycles) are preferred.\n",
    "        # If data is noisy or exhibits random walk behavior, RWWD and ARIMA are excellent choices. AutoETS allows automatic selection between simple and double exponential smoothing.\n",
    "        # HistoricAverage acts as a fallback for stationary, mean-reverting series.\n",
    "\n",
    "        Naive(),\n",
    "        HistoricAverage(),\n",
    "        RandomWalkWithDrift(),\n",
    "        AutoETS(model='ZAN', alias='AutoETS_SimpleDouble'),\n",
    "        AutoARIMA(season_length=1, alias='ARIMA')\n",
    "    ],\n",
    "    5: [\n",
    "        # Category 5: All other cases\n",
    "        # Models:\n",
    "        # - Naive: Always included for benchmarking.\n",
    "        # - ARIMA: Adaptable model for data with autocorrelation and trend, without requiring seasonality.\n",
    "        # - AutoETS (Simple/Double, damped trend): Offers flexibility across level and trend modeling with exponential smoothing, including damped trend components for stabilizing long-term forecasts.\n",
    "        # - HistoricAverage: Provides robustness in noisy datasets or when structure is unclear.\n",
    "        # - RandomWalkWithDrift: Simple trend model, less prone to overfitting, useful when trend is present but weak.\n",
    "        # - Reasoning: When the structure of the series is uncertain or doesn't fit neatly into seasonal/non-seasonal classifications, we need versatile models. ARIMA and AutoETS can adapt to various patterns.\n",
    "        # Damped trends are especially useful when the series appears to trend but might plateau. Simpler models like Naive, HistoricAverage, and RWWD help prevent overfitting and offer interpretability.\n",
    "        \n",
    "        Naive(),\n",
    "        AutoARIMA(season_length=12, alias='ARIMA'),\n",
    "        AutoETS(model='ZAN', alias='AutoETS_SimpleDouble'),\n",
    "        HistoricAverage(),\n",
    "        RandomWalkWithDrift()\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Assign models to each time series based on its category\n",
    "ts_models = {}\n",
    "for uid in data['unique_id'].unique():\n",
    "    ts = data[data['unique_id'] == uid]['y']\n",
    "    cat = get_category(ts)\n",
    "    ts_models[uid] = category_models[cat]\n",
    "    print(f\"Series {uid} assigned to Category {cat} with models: {[type(m).__name__ for m in ts_models[uid]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5714c477",
   "metadata": {},
   "source": [
    "# d) Perform a cross validation simulating forecasts for one year, every half year for an evaluation period of 2 years. Do not perform cross-validation for time series shorter than 48 observations.\n",
    "# e) Calculate the accuracy for all models over all errors with the metric chosen by the user. If at least one value of a time series is zeroes, throw a warning if MAPE is used.\n",
    "# f) The system should now select the best performing single forecasting method per time series according to the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74cc42de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best model for C1002: RWD with avg MAPE = 0.0803\n",
      "📊 Naive benchmark for C1002: avg MAPE = 0.0604\n",
      "\n",
      "\n",
      "✅ Best model for C1016: RWD with avg MAPE = 0.0840\n",
      "📊 Naive benchmark for C1016: avg MAPE = 0.1214\n",
      "\n",
      "\n",
      "⏩ Skipping C1050 (only 22 observations)\n",
      "\n",
      "✅ Best model for C1080: SeasWA with avg MAPE = 0.0692\n",
      "📊 Naive benchmark for C1080: avg MAPE = 0.0920\n",
      "\n",
      "\n",
      "✅ Best model for C1126: ARIMA with avg MAPE = 0.1262\n",
      "📊 Naive benchmark for C1126: avg MAPE = 0.1379\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from statsforecast import StatsForecast\n",
    "from utilsforecast.evaluation import evaluate\n",
    "import utilsforecast.losses as ufl\n",
    "from statsforecast.models import Naive\n",
    "\n",
    "# Define key parameters\n",
    "forecast_horizon = 12          # Number of periods to forecast\n",
    "min_obs = 48                   # Minimum number of observations required to apply evaluation\n",
    "rolling_step = 6               # Step size for rolling evaluation\n",
    "eval_window = 24               # Total number of test observations (split into rolling windows)\n",
    "best_models = {}               # Dictionary to store the best model per time series\n",
    "benchmark_results = {}         # Dictionary to store Naive benchmark error per series\n",
    "\n",
    "# Metric selected by the user\n",
    "# List of available evaluation metrics\n",
    "all_metrics = [ufl.mape, ufl.mse]\n",
    "\n",
    "# Iterate over each unique time series\n",
    "for uid in data['unique_id'].unique():\n",
    "    ts_data = data[data['unique_id'] == uid].copy()\n",
    "    ts_data = ts_data.sort_values('ds')\n",
    "    y_values = ts_data['y'].reset_index(drop=True)\n",
    "\n",
    "    # Skip time series with fewer than 48 observations\n",
    "    if len(y_values) < min_obs:\n",
    "        print(f\"⏩ Skipping {uid} (only {len(y_values)} observations)\\n\")\n",
    "        continue\n",
    "\n",
    "    # Warn if using MAPE and the series contains zero values\n",
    "    if metric == 'mape' and (y_values == 0).any():\n",
    "        warnings.warn(f\"⚠️ MAPE selected but series {uid} contains zero values.\\n\")\n",
    "    \n",
    "    models = ts_models[uid]  # Retrieve models assigned to this time series\n",
    "\n",
    "    # Perform time series cross-validation using rolling windows\n",
    "    sf = StatsForecast(models=models, freq='MS', n_jobs=1)\n",
    "    cv_df = sf.cross_validation(\n",
    "        df=ts_data,\n",
    "        step_size=rolling_step,\n",
    "        n_windows=(eval_window - forecast_horizon)//rolling_step + 1,\n",
    "        h=forecast_horizon\n",
    "    )\n",
    "\n",
    "    # Evaluate forecasting accuracy using selected metrics\n",
    "    results_df = evaluate(\n",
    "        df=cv_df.drop(columns='cutoff'), \n",
    "        train_df=ts_data, \n",
    "        metrics=all_metrics\n",
    "    )\n",
    "\n",
    "    # Keep only rows corresponding to the selected metric (e.g., 'mape' or 'mse')\n",
    "    model_errors = results_df[results_df['metric'] == metric]\n",
    "    model_errors_long = model_errors.melt(\n",
    "        id_vars=['unique_id', 'metric'], \n",
    "        var_name='model', \n",
    "        value_name='value'\n",
    "    )\n",
    "    avg_errors = model_errors_long.groupby('model')['value'].mean().reset_index()\n",
    "\n",
    "    # Skip this series if no results were computed\n",
    "    if avg_errors.empty:\n",
    "        print(f\"⚠️ No evaluation results for {uid}\")\n",
    "        continue\n",
    "\n",
    "    # Select the best model (excluding Naive)\n",
    "    non_naive = avg_errors[avg_errors['model'] != 'Naive']\n",
    "    best_model = non_naive.sort_values('value').iloc[0]\n",
    "\n",
    "    # Retrieve the error of the Naive model\n",
    "    naive_error = avg_errors[avg_errors['model'] == 'Naive']['value'].values[0]\n",
    "\n",
    "    # Store results for this time series\n",
    "    best_models[uid] = best_model\n",
    "    benchmark_results[uid] = naive_error\n",
    "\n",
    "    # Display results\n",
    "    print(f\"✅ Best model for {uid}: {best_model['model']} with avg {metric.upper()} = {best_model['value']:.4f}\")\n",
    "    print(f\"📊 Naive benchmark for {uid}: avg {metric.upper()} = {naive_error:.4f}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09035b2",
   "metadata": {},
   "source": [
    "# g) Now forecast into the future and only select the chosen forecast method per time series. Try to make this as efficient as possible.\n",
    "# h) Create the following outputs as print for each time series: Name of the time series, chosen model, accuracy metric and accuracy value of the chosen forecast method and the Naive as benchmark.\n",
    "# i) Save the forecasts as .csv file in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "249ee9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Forecast summary for C1002:\n",
      "   ✅ Chosen Model: RWD\n",
      "   🔍 Accuracy (MAPE): 0.0803\n",
      "   🧪 Naive Benchmark (MAPE): 0.0604\n",
      "\n",
      "📈 Forecast summary for C1016:\n",
      "   ✅ Chosen Model: RWD\n",
      "   🔍 Accuracy (MAPE): 0.0840\n",
      "   🧪 Naive Benchmark (MAPE): 0.1214\n",
      "\n",
      "📈 Forecast summary for C1080:\n",
      "   ✅ Chosen Model: SeasWA\n",
      "   🔍 Accuracy (MAPE): 0.0692\n",
      "   🧪 Naive Benchmark (MAPE): 0.0920\n",
      "\n",
      "📈 Forecast summary for C1126:\n",
      "   ✅ Chosen Model: ARIMA\n",
      "   🔍 Accuracy (MAPE): 0.1262\n",
      "   🧪 Naive Benchmark (MAPE): 0.1379\n",
      "\n",
      "Forecast summary saved as 'forecast_summary.csv'.\n"
     ]
    }
   ],
   "source": [
    "# g) Forecast the future using only the best model per series\n",
    "final_forecasts = []  # List to store final forecasts in Nixtla format\n",
    "\n",
    "for uid in data['unique_id'].unique():\n",
    "    if uid not in best_models:\n",
    "        continue  # Skip if no best model was selected for this series\n",
    "\n",
    "    ts_data = data[data['unique_id'] == uid].copy()\n",
    "    ts_data = ts_data.sort_values('ds')  # Ensure correct chronological order\n",
    "\n",
    "    best_model_name = best_models[uid]['model']\n",
    "    \n",
    "    # Find the corresponding model object by its string representation\n",
    "    model_obj = next((m for m in ts_models[uid] if m.__repr__() == best_model_name), None)\n",
    "    if model_obj is None:\n",
    "        print(f\"❌ Model {best_model_name} not found for {uid}\")\n",
    "        continue\n",
    "\n",
    "    # Forecast using only the selected model\n",
    "    sf = StatsForecast(models=[model_obj], freq='MS', n_jobs=1)\n",
    "    forecast_df = sf.forecast(df=ts_data, h=forecast_horizon)\n",
    "\n",
    "    # Ensure consistent format (although unique_id is already set)\n",
    "    forecast_df['unique_id'] = uid\n",
    "    final_forecasts.append(forecast_df)\n",
    "\n",
    "    # h) Print summary of the chosen model and its performance\n",
    "    print(f\"📈 Forecast summary for {uid}:\")\n",
    "    print(f\"   ✅ Chosen Model: {best_model_name}\")\n",
    "    print(f\"   🔍 Accuracy ({metric.upper()}): {best_models[uid]['value']:.4f}\")\n",
    "    print(f\"   🧪 Naive Benchmark ({metric.upper()}): {benchmark_results[uid]:.4f}\\n\")\n",
    "\n",
    "\n",
    "# Create summary DataFrame from best_models and benchmark_results\n",
    "summary_data = []\n",
    "\n",
    "for uid, model_info in best_models.items():\n",
    "    summary_data.append({\n",
    "        'time_series': uid,\n",
    "        'model': model_info['model'],\n",
    "        'accuracy': round(model_info['value'], 4),\n",
    "        'naive_benchmark': round(benchmark_results[uid], 4)\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Save the summary table to CSV\n",
    "summary_df.to_csv(f'forecast_summary_{metric}.csv', index=False)\n",
    "\n",
    "print(\"Forecast summary saved as 'forecast_summary.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0faa32d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'boxcox_inverse' from 'coreforecast.scalers' (/opt/anaconda3/lib/python3.12/site-packages/coreforecast/scalers.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcoreforecast\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscalers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m boxcox, boxcox_lambda, boxcox_inverse\n\u001b[1;32m      9\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNIXTLA_ID_AS_COL\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstatsforecast\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     12\u001b[0m     Naive,\n\u001b[1;32m     13\u001b[0m     HistoricAverage,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     AutoARIMA\n\u001b[1;32m     20\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'boxcox_inverse' from 'coreforecast.scalers' (/opt/anaconda3/lib/python3.12/site-packages/coreforecast/scalers.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import sys\n",
    "import warnings\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from coreforecast.scalers import boxcox, boxcox_lambda, boxcox_inverse\n",
    "os.environ['NIXTLA_ID_AS_COL'] = '1'\n",
    "\n",
    "from statsforecast.models import (\n",
    "    Naive,\n",
    "    HistoricAverage,\n",
    "    RandomWalkWithDrift,\n",
    "    SeasonalNaive,\n",
    "    SeasonalWindowAverage,\n",
    "    AutoETS,\n",
    "    HoltWinters,\n",
    "    AutoARIMA\n",
    ")\n",
    "from statsforecast import StatsForecast\n",
    "from utilsforecast.evaluation import evaluate\n",
    "import utilsforecast.losses as ufl\n",
    "\n",
    "# USER INPUT\n",
    "file_name = 'PROJECT/proj1_exampleinput.csv'\n",
    "time_series = ['C1002', 'C1050', 'C1080', 'C1016', 'C3029']\n",
    "metric = 'mape'  # 'mape' or 'mse'\n",
    "\n",
    "\n",
    "# Verify if a valid metric was selected\n",
    "valid_metrics = ['mape', 'mse']\n",
    "if metric not in valid_metrics:\n",
    "    raise ValueError(f\"Invalid metric. Please choose either 'mape' or 'mse'.\")\n",
    "\n",
    "# a) READ THE DATA\n",
    "try:\n",
    "    ds = pd.read_csv(file_name, parse_dates=['Month'])\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_name}' was not found.\")\n",
    "    sys.exit(1) # Exit the script if file is not found\n",
    "\n",
    "\n",
    "# b) Filter only the time series selected by the user, sorting everything by ID and date, renaming the columns so they match what the StatsForecast library expects.\n",
    "\n",
    "\n",
    "ds = ds[ds['product_class'].isin(time_series)]\n",
    "ds = ds.sort_values(by=['product_class', 'Month'])\n",
    "ds = ds.rename(columns={'product_class': 'unique_id', 'Month': 'ds', 'sales_volume': 'y'})\n",
    "ds = ds.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "#Handling Missing Dates\n",
    "\n",
    "# Store DataFrames for each unique_id after handling missing dates\n",
    "processed_dfs = []\n",
    "\n",
    "# Iterate over each unique product_class (now 'unique_id')\n",
    "for uid in ds['unique_id'].unique():\n",
    "    subset_df = ds[ds['unique_id'] == uid].copy()\n",
    "\n",
    "    # Determine the full date range for this unique_id\n",
    "    min_date = subset_df['ds'].min()\n",
    "    max_date = subset_df['ds'].max()\n",
    "    full_date_range = pd.date_range(start=min_date, end=max_date, freq='MS') # 'MS' for Month Start\n",
    "\n",
    "    # Create a new DataFrame with the full date range and the unique_id\n",
    "    # Then merge it with the original data for this unique_id\n",
    "    reindexed_df = pd.DataFrame({'ds': full_date_range, 'unique_id': uid})\n",
    "    reindexed_df = pd.merge(reindexed_df, subset_df, on=['unique_id', 'ds'], how='left')\n",
    "\n",
    "    # Fill 'y' (sales_volume) for missing dates with 0\n",
    "    missing_dates_count = reindexed_df['y'].isnull().sum()\n",
    "    if missing_dates_count > 0:\n",
    "        print(f\"Product Class: {uid}\")\n",
    "        missing_dates = reindexed_df[reindexed_df['y'].isnull()]['ds']\n",
    "        for date in missing_dates:\n",
    "            print(f\"  Missing Date: {date.strftime('%Y-%m-%d')}, Sales Volume filled with: 0\")\n",
    "        reindexed_df['y'] = reindexed_df['y'].fillna(0)\n",
    "    else:\n",
    "        print(f\"Product Class: {uid} - No implicit missing dates found.\")\n",
    "\n",
    "    processed_dfs.append(reindexed_df)\n",
    "\n",
    "# Concatenate all processed DataFrames back into a single DataFrame\n",
    "ds = pd.concat(processed_dfs, ignore_index=True)\n",
    "ds = ds.sort_values(by=['unique_id', 'ds']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "#c) MODEL SELECTION BY CATEGORY\n",
    "\n",
    "# This function helps us figure out if a time series has strong seasonality.\n",
    "# It first checks if there's enough data to do a proper seasonal decomposition (at least two full cycles).\n",
    "# If that condition is met, it tries to break the series into three parts: trend, seasonality, and leftover noise (residuals),\n",
    "# An additive model was selected because it works better when the data has zeros or stays around the same range.\n",
    "# Then, it compares how much the seasonal part varies compared to the noise.\n",
    "# If the seasonal variation is clearly stronger (we're using 2x as a threshold), we say the series has high seasonality.\n",
    "# If there's not enough data or the decomposition fails, we catch the error and just assume there's no strong seasonality.\n",
    "\n",
    "def high_seasonality(series, period=12):\n",
    "    if len(series) < 2 * period:\n",
    "        return False\n",
    "    try:\n",
    "        result = seasonal_decompose(series, model='additive', period=period, extrapolate_trend='freq')\n",
    "        seasonal_var = np.var(result.seasonal.dropna())\n",
    "        resid_var = np.var(result.resid.dropna())\n",
    "        return seasonal_var > resid_var * 2\n",
    "    except Exception as e:\n",
    "        print(f\"Seasonal decomposition failed. Error: {e}\")\n",
    "        return False\n",
    "\n",
    "#Checks if all values in the series are positive.\n",
    "def is_positive(series):\n",
    "    return series.min() > 0\n",
    "\n",
    "#Checks if the series contains any zero values.\n",
    "def zeroes_in_dataframe(series):\n",
    "    return (series == 0).any()\n",
    "\n",
    "    \n",
    "#Assigning time series to a category\n",
    "def get_category(ts_data_for_series, period=12):\n",
    "    series_values = ts_data_for_series['y']\n",
    "    if len(series_values) < 2 * period: # Category 1: shorter than two seasonality periods (24 months)\n",
    "        return 1\n",
    "    elif zeroes_in_dataframe(series_values) and high_seasonality(series_values, period):\n",
    "        return 2 # Category 2: zeroes and high seasonality\n",
    "    elif high_seasonality(series_values, period) and is_positive(series_values):\n",
    "        return 3 # Category 3: high seasonality and only positive values\n",
    "    elif not high_seasonality(series_values, period) and zeroes_in_dataframe(series_values):\n",
    "        return 4 # Category 4: low/no seasonality and zeroes\n",
    "    else:\n",
    "        return 5 # Category 5: All remaining cases\n",
    "\n",
    "\n",
    "\n",
    "category_models = {\n",
    "    1: [\n",
    "        # Category 1: Shorter than two seasonality periods\n",
    "        # Models:\n",
    "        # - Naive: Serves as a benchmark for all forecasting tasks. It assumes the next value is the last observed one. Useful when data is limited.\n",
    "        # - HistoricAverage: Calculates the mean of historical values. Useful when no seasonal or trend patterns are observable due to short history.\n",
    "        # Reasoning: Because there is not enough data, complex models may not be able to capture seasonality or trends optimally. Simple models can work with short ranges.\n",
    "        \n",
    "        Naive(),\n",
    "        HistoricAverage()\n",
    "        ],\n",
    "    2: [\n",
    "        #Category 2: High seasonality + zero values\n",
    "        # Models:\n",
    "        # - Naive\n",
    "        # - SeasonalNaive: Assumes that the current value will be the same as in the same period of the previous season. \n",
    "        # - SeasonalWindowAverage: Averages values from the same point in past seasons to smooth out the seasonality. \n",
    "        # - SARIMA: A model that can handle both trend and seasonality. It combines autoregression, moving averages, and seasonal differencing to adapt to complex patterns.\n",
    "        # - Holt-Winters (Additive): Used for series with both trend and seasonality, especially when values can drop to zero. The additive version works better than the multiplicative one in these cases, because it doesn’t break when zeros appear.\n",
    "        # Reasoning:\n",
    "        # In this scenario, we have strong seasonal patterns and some zero values in the data. That makes additive models a safer choice, because multiplicative ones can’t handle zeros properly.\n",
    "        # We use SeasonalNaive and SeasonalWindowAverage because they take advantage of the repeating seasonal patterns. SARIMA and Holt-Winters (Additive) are more flexible, because they can also catch trends along with seasonality.\n",
    "\n",
    "\n",
    "        Naive(),\n",
    "        SeasonalNaive(season_length=12),\n",
    "        SeasonalWindowAverage(window_size=3, season_length=12),\n",
    "        AutoARIMA(season_length=12, alias='SARIMA'),\n",
    "        HoltWinters(season_length=12, error_type='A', alias='HW_Add')\n",
    "        ],\n",
    "    3: [\n",
    "        # Category 3: High seasonality + positive values\n",
    "        # Models:\n",
    "        # - Naive\n",
    "        # - SeasonalNaive\n",
    "        # - SeasonalWindowAverage\n",
    "        # - SARIMA\n",
    "        # - AutoETS: Automatically selects between additive and multiplicative exponential smoothing.\n",
    "        # - Reasoning: With strong seasonality and no zero values, multiplicative models become viable. SeasonalNaive and SeasonalWindowAverage are included because they look at what happened in the same period during past seasons, which helps when patterns repeat over time.\n",
    "        # SARIMA is included because it is a flexible model that can capture seasonality, trends and complex relationships in the data, such as autocorrelation, moving average effects, and seasonal differencing to remove seasonal trends.\n",
    "        # AutoETS is a model that picks the best way to handle the data, and since all values are positive, it can also use multiplicative components that adjust better when seasonal effects grow with the data.\n",
    "        \n",
    "        Naive(),\n",
    "        SeasonalNaive(season_length=12),\n",
    "        SeasonalWindowAverage(window_size=3, season_length=12),\n",
    "        AutoARIMA(season_length=12, alias='SARIMA'),\n",
    "        AutoETS(model='ZZZ', alias='AutoETS')],\n",
    "    4: [\n",
    "        # Category 4: Low seasonality + zero values\n",
    "        # Models:\n",
    "        # - Naive\n",
    "        # - HistoricAverage\n",
    "        # - RandomWalkWithDrift: Assumes the series follows a trend with random variation. Useful when there's no seasonality but a clear directional movement.\n",
    "        # - AutoETS (Simple/Double) Simple Exponential Smoothing assumes no trend/seasonality. Double Exponential Smoothing introduces trend while maintaining simplicity.\n",
    "        # - ARIMA: General-purpose model that can capture linear trends and autocorrelation without requiring seasonality.\n",
    "        # - Reasoning: In the absence of strong seasonality, models that focus on level and trend (rather than seasonal cycles) are better.\n",
    "        # If data is noisy or exhibits random walk behavior, RWWD and ARIMA are excellent choices. AutoETS allows automatic selection between simple and double exponential smoothing.\n",
    "        # HistoricAverage acts as a fallback for stationary, mean-reverting series.\n",
    "\n",
    "        Naive(),\n",
    "        HistoricAverage(),\n",
    "        RandomWalkWithDrift(),\n",
    "        AutoETS(model='ZAN', alias='AutoETS_SimpleDouble'),\n",
    "        AutoARIMA(season_length=1, alias='ARIMA')\n",
    "        ],\n",
    "    5: [\n",
    "        # Category 5: All other cases\n",
    "        # Models:\n",
    "        # - Naive\n",
    "        # - ARIMA: Adaptable model for data with autocorrelation and trend, without requiring seasonality.\n",
    "        # - AutoETS (Simple/Double, damped trend): Offers flexibility across level and trend modeling with exponential smoothing, including damped trend components for stabilizing long-term forecasts.\n",
    "        # - HistoricAverage: Provides robustness in noisy datasets or when structure is unclear.\n",
    "        # - RandomWalkWithDrift: Simple trend model, less prone to overfitting, useful when trend is present but weak.\n",
    "        # - Reasoning: When the structure of the series is uncertain or doesn't fit neatly into seasonal/non-seasonal classifications, we need versatile models. ARIMA and AutoETS can adapt to various patterns.\n",
    "        # Damped trends are especially useful when the series appears to trend but might plateau. Simpler models like Naive, HistoricAverage, and RWWD help prevent overfitting and offer interpretability.\n",
    "        \n",
    "        Naive(),\n",
    "        AutoARIMA(season_length=12, alias='ARIMA'),\n",
    "        AutoETS(model='ZAN', alias='AutoETS_SimpleDouble'),\n",
    "        HistoricAverage(),\n",
    "        RandomWalkWithDrift()]\n",
    "}\n",
    "\n",
    "# Assign models to each time series based on its category\n",
    "\n",
    "print(\"\\n---------------------------------------------------- Model Assignment-------------------------------------------------------\\n \")\n",
    "\n",
    "ts_models = {}\n",
    "for uid in ds['unique_id'].unique():\n",
    "    ts = ds[ds['unique_id'] == uid]\n",
    "    cat = get_category(ts)\n",
    "    ts_models[uid] = category_models[cat]\n",
    "    print(f\"Series {uid} assigned to Category {cat} with models: {[type(m).__name__ for m in ts_models[uid]]}\")\n",
    "\n",
    "print(\"----------------------------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "# d, e, f) Cross-validation, Evaluation, Best Model Selection\n",
    "# Define key parameters\n",
    "forecast_horizon = 12          # Number of periods to forecast (one year)\n",
    "min_obs = 48                   # Minimum number of observations required for cross-validation (4 years of monthly data for a 1-year forecast window)\n",
    "rolling_step = 6               # Step size for rolling evaluation (every half year)\n",
    "eval_window = 24               # Total number of test observations (evaluation period of 2 years)\n",
    "best_models = {}               # Dictionary to store the best model per time series\n",
    "benchmark_results = {}         # Dictionary to store Naive benchmark error per series\n",
    "boxcox_lambdas = {}\n",
    "\n",
    "# List of available evaluation metrics\n",
    "all_metrics = [ufl.mape, ufl.mse]\n",
    "\n",
    "print(\"\\n-------------------------------------------- Cross-Validation and Evaluation -----------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "# This part of the code takes care of handling each time series separately. For every unique_id, it grabs all the related data, sorts it by time.\n",
    "\n",
    "for uid in ds['unique_id'].unique():\n",
    "    ts_data = ds[ds['unique_id'] == uid].copy()\n",
    "    ts_data = ts_data.sort_values('ds')\n",
    "    y_values = ts_data['y'].reset_index(drop=True)\n",
    "\n",
    "    # Skip time series with fewer than min_obs observations for cross-validation\n",
    "    if len(y_values) < min_obs:\n",
    "        print(f\"Skipping {uid} (only {len(y_values)} observations, less than {min_obs} required for CV)\\n\")\n",
    "        continue\n",
    "\n",
    "    # Warn if using MAPE and the series contains zero values, as MAPE is undefined for zeros\n",
    "    if metric == 'mape' and (y_values == 0).any():\n",
    "        warnings.warn(f\"MAPE selected but series {uid} contains zero values. MAPE might be inaccurate or infinite.\\n\")\n",
    "    \n",
    "    models = ts_models[uid]  # Retrieve models assigned to this time series\n",
    "\n",
    "\n",
    "    #BOX COX TRANSFORMATION FOR TIME SERIES WITHOUT ZERO VALUES\n",
    "    ts_data_for_cv = ts_data.copy()\n",
    "    current_lambda = None\n",
    "    is_series_positive = is_positive(ts_data['y'])\n",
    "    has_autoarima_model = any(isinstance(m, AutoARIMA) for m in models)\n",
    "\n",
    "    if is_series_positive and has_autoarima_model:\n",
    "        try:\n",
    "\n",
    "            current_lambda = boxcox_lambda(ts_data['y'].values, season_length=12, method='guerrero')\n",
    "            ts_data_for_cv['y'] = boxcox(ts_data['y'].values, lmbda=current_lambda)\n",
    "            boxcox_lambdas[uid] = current_lambda\n",
    "            print(f\"Applying Box Cox transformation to'{uid}' with lambda: {current_lambda:.4f}\")\n",
    "        except Exception as exception:\n",
    "            print(f\"Box Cox transformation failed  {uid}. Error: {exception}. Proceeding without transformation.\")\n",
    "            current_lambda = None\n",
    "            boxcox_lambdas[uid] = None\n",
    "    else:\n",
    "        boxcox_lambdas[uid] = None\n",
    "\n",
    "    # Perform time series cross-validation using rolling windows\n",
    "    # n_windows calculates how many times we can roll the window over the eval_window period.\n",
    "    # (+1 because n_windows is inclusive of the first window)\n",
    "    n_windows_calculated = (eval_window - forecast_horizon) // rolling_step + 1\n",
    "    \n",
    "    try:\n",
    "        sf = StatsForecast(models=models, freq='MS', n_jobs=1) # freq='MS' for monthly start\n",
    "        cv_df = sf.cross_validation(\n",
    "            df=ts_data,\n",
    "            step_size=rolling_step,\n",
    "            n_windows=n_windows_calculated,\n",
    "            h=forecast_horizon\n",
    "        )\n",
    "        \n",
    "        cv_df = cv_df.reset_index()\n",
    "    except Exception as error:\n",
    "        print(f\"Cross-validation failed for {uid}. Error: {error}\\n\")\n",
    "        continue\n",
    "\n",
    "    #BOX COX INVERSE\n",
    "    if current_lambda is not None:\n",
    "        for model_obj in models:\n",
    "            model_col_name = model_obj.alias if hasattr(model_obj, 'alias') else type(model_obj).__name__\n",
    "            if isinstance(model_obj, AutoARIMA) and model_col_name in cv_df.columns:\n",
    "                try:\n",
    "                    cv_df[model_col_name] = boxcox_inverse(cv_df[model_col_name].values, lmbda=current_lambda)\n",
    "                    cv_df[model_col_name] = cv_df[model_col_name].clip(lower=0)\n",
    "                except Exception as exception:\n",
    "                    print(f\"Inverse Box Cox Failed {uid} (model {model_col_name}). Error: {exception}\")\n",
    "    \n",
    "\n",
    "    # Evaluate forecasting accuracy using selected metrics\n",
    "    # Drop 'cutoff' column as it's not needed for evaluation\n",
    "    results_df = evaluate(\n",
    "        df=cv_df.drop(columns='cutoff'), \n",
    "        train_df=ts_data, # train_df is needed for some metrics like MAPE\n",
    "        metrics=all_metrics\n",
    "    )\n",
    "\n",
    "    # Filter evaluation results to keep only the user-selected metric\n",
    "    model_errors = results_df[results_df['metric'] == metric]\n",
    "    \n",
    "    # Melt the DataFrame to easily group by model and calculate average errors\n",
    "    model_errors_long = model_errors.melt(\n",
    "        id_vars=['unique_id', 'metric'], \n",
    "        var_name='model', \n",
    "        value_name='value'\n",
    "    )\n",
    "    # Calculate the average error for each model across all windows\n",
    "    avg_errors = model_errors_long.groupby('model')['value'].mean().reset_index()\n",
    "\n",
    "    # Skip this series if no evaluation results were computed (e.g., if model failed)\n",
    "    if avg_errors.empty:\n",
    "        print(f\"No evaluation results for {uid} after filtering by metric. Skipping.\\n\")\n",
    "        continue\n",
    "\n",
    "    # Select the best model (excluding Naive from the \"best model\" selection, but keeping it for benchmark)\n",
    "    non_naive_models = avg_errors[avg_errors['model'] != 'Naive']\n",
    "    best_model = non_naive_models.sort_values('value').iloc[0]\n",
    "\n",
    "    # Retrieve the error of the Naive model for benchmarking\n",
    "    naive_error = avg_errors[avg_errors['model'] == 'Naive']['value'].values[0]\n",
    "\n",
    "    # Store results for this time series\n",
    "    best_models[uid] = best_model\n",
    "    benchmark_results[uid] = naive_error\n",
    "\n",
    "    # Display results for each time series\n",
    "    print(f\"Best model for {uid}: {best_model['model']} with avg {metric.upper()} = {best_model['value']:.4f}\")\n",
    "    print(f\"Naive benchmark for {uid}: avg {metric.upper()} = {naive_error:.4f}\\n\")\n",
    "print(\"----------------------------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "#g, h, i) Forecast into the future and Save Outputs\n",
    "final_forecasts_list = []\n",
    "\n",
    "print(\"\\n-------------------------------------------- Final Forecasting and Summary -------------------------------------------------\\n\")\n",
    "# Iterate through each unique time series ID present in the data.\n",
    "for uid in ds['unique_id'].unique():\n",
    "    # Check if a best model was selected for the current time series (uid) during cross-validation.\n",
    "    if uid not in best_models:\n",
    "        print(f\"Skipping final forecast for {uid} (was skipped during CV/evaluation).\\n\")\n",
    "        continue # Move to the next time series.\n",
    "\n",
    "    # Extract the data for the current time series and ensure it's sorted chronologically.\n",
    "    ts_data = ds[ds['unique_id'] == uid].copy().sort_values('ds')\n",
    "\n",
    "    # Retrieve the information about the best performing model for this time series.\n",
    "    best_model_info = best_models[uid]\n",
    "    best_model_name = best_model_info['model']\n",
    "    \n",
    "    # Find the model from the ts_models\n",
    "    model_obj = next((m for m in ts_models[uid] if m.__repr__() == best_model_name), None)\n",
    "    if model_obj is None:\n",
    "        print(f\"Model object for {best_model_name} not found for {uid}. Cannot forecast.\")\n",
    "        continue # Move to the next time series if the model object can't be found.\n",
    "    \n",
    "    #Box Cox Transformation\n",
    "    ts_data_for_forecast = ts_data.copy()\n",
    "    current_lambda = boxcox_lambdas.get(uid)\n",
    "    is_best_model_autoarima = isinstance(model_obj, AutoARIMA)\n",
    "    if is_best_model_autoarima and current_lambda is not None:\n",
    "        try:\n",
    "            ts_data_for_forecast['y'] = boxcox(ts_data['y'].values, lmbda=current_lambda)\n",
    "            print(f\"Aplicando transformación Box-Cox a los datos de entrada para la previsión final de '{uid}' con el mejor modelo '{best_model_name}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Advertencia: La transformación Box-Cox para la entrada de la previsión final falló para {uid}. Error: {e}. Procediendo con los datos originales.\")\n",
    "            ts_data_for_forecast = ts_data.copy() # Vuelve a los datos originales si la transformación falla.\n",
    "    # FIN DE MODIFICACIÓN\n",
    "\n",
    "    # Final forecast using only the selected best model.\n",
    "    try:\n",
    "        sf = StatsForecast(models=[model_obj], freq='MS', n_jobs=1)\n",
    "        forecast_df = sf.forecast(df=ts_data, h=forecast_horizon).reset_index()\n",
    "        if is_best_model_autoarima and current_lambda is not None:\n",
    "            forecast_col_name = model_obj.alias if hasattr(model_obj, 'alias') else type(model_obj).__name__\n",
    "            if forecast_col_name in forecast_df.columns:\n",
    "                try:\n",
    "                    forecast_df[forecast_col_name] = boxcox_inverse(forecast_df[forecast_col_name].values, lmbda=current_lambda)\n",
    "                    forecast_df[forecast_col_name] = forecast_df[forecast_col_name].clip(lower=0)\n",
    "                    print(f\"Aplicada la transformación inversa Box-Cox a los pronósticos finales para '{uid}'.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Advertencia: La transformación inversa Box-Cox para los pronósticos finales falló para {uid}. Error: {e}\")\n",
    "    except Exception as error:\n",
    "        print(f\"Error during final forecast for {uid}, model {best_model_name}: {error}\")\n",
    "        continue\n",
    "\n",
    "    \n",
    "    print(f\"Forecast summary for {uid}:\")\n",
    "    print(f\"   Chosen Model: {best_model_name}\")\n",
    "    print(f\"   Accuracy ({metric.upper()}): {best_model_info['value']:.4f}\")\n",
    "    print(f\"   Naive Benchmark ({metric.upper()}): {benchmark_results[uid]:.4f}\\n\")\n",
    "\n",
    "# i) Save the forecasts as .csv file\n",
    "\n",
    "if final_forecasts_list:\n",
    "    current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    file_path = os.path.join(current_dir, f'future_forecasts_{metric}.csv')\n",
    "    pd.concat(final_forecasts_list, ignore_index=True).to_csv(file_path, index=False)\n",
    "    print(f\"Forecasts saved as '{file_path}'.\")\n",
    "else:\n",
    "    print(\"No future forecasts were generated or saved.\")\n",
    "print(\"----------------------------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
